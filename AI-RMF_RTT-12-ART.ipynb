{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyQna+rMet85j846ZfwhkJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"89f50a548abc420b884bc689562ce5f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efc257db5edb4523b380c48939c64b78","IPY_MODEL_7b875deffa61414baec6550e0b8b533b","IPY_MODEL_cda315b76979413a82f69a079f7c8aa9"],"layout":"IPY_MODEL_93b8d64bf6a2443395521bd1ea96e3ec"}},"efc257db5edb4523b380c48939c64b78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_494d7e171986438f85a8f81766a22f13","placeholder":"​","style":"IPY_MODEL_a084058d58ed40349a41e4d29e8e5345","value":"PGD - Batches: "}},"7b875deffa61414baec6550e0b8b533b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cd50022cc7b4ad485d23765f5642d95","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb2b88a2a1cd45c291348b54ef04944a","value":1}},"cda315b76979413a82f69a079f7c8aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23b951f0a5b64e6fafde2acd615e8e94","placeholder":"​","style":"IPY_MODEL_863cce36e49a4c13a0b71cc6a4e39583","value":" 4/? [00:00&lt;00:00,  4.46it/s]"}},"93b8d64bf6a2443395521bd1ea96e3ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"494d7e171986438f85a8f81766a22f13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a084058d58ed40349a41e4d29e8e5345":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cd50022cc7b4ad485d23765f5642d95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cb2b88a2a1cd45c291348b54ef04944a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"23b951f0a5b64e6fafde2acd615e8e94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"863cce36e49a4c13a0b71cc6a4e39583":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd989e2420ae4db2b2d361e44f284a3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b19b1992bc54ad790d1ad25ae4a82f3","IPY_MODEL_88993c60ae4347b8aed81422bc3fb3a5","IPY_MODEL_096b7720aa4c4d7599839ffdd24f35b4"],"layout":"IPY_MODEL_a6c45a00d0e94b5cbf19d0f0dfc122be"}},"5b19b1992bc54ad790d1ad25ae4a82f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be7b8b5f98bd40f780a2b6c089c4e9b4","placeholder":"​","style":"IPY_MODEL_b9339025d63a4003b14eb713f0ed86e0","value":"DeepFool: 100%"}},"88993c60ae4347b8aed81422bc3fb3a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2d8efa2145643de990f5e3701780a42","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4494cae6b744ce3b2d3328c12c0b2ca","value":100}},"096b7720aa4c4d7599839ffdd24f35b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba2786d36244d07b1df1fa8f87d0e48","placeholder":"​","style":"IPY_MODEL_cc2e2291c4504d0d90f732e01bd2ae76","value":" 100/100 [01:58&lt;00:00,  1.01s/it]"}},"a6c45a00d0e94b5cbf19d0f0dfc122be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be7b8b5f98bd40f780a2b6c089c4e9b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9339025d63a4003b14eb713f0ed86e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2d8efa2145643de990f5e3701780a42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4494cae6b744ce3b2d3328c12c0b2ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ba2786d36244d07b1df1fa8f87d0e48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc2e2291c4504d0d90f732e01bd2ae76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf1c5959ba1b4c0587fc42fd43dd2c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90455059f76141aabe5b6e1e699f4b21","IPY_MODEL_62b3b7e688d14b02a736f83d49bd65bc","IPY_MODEL_57b1f1fc36bf4a69b1039a2a8c6df196"],"layout":"IPY_MODEL_1732e90a0033490ca3b2f899c7c190f7"}},"90455059f76141aabe5b6e1e699f4b21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b684dc1f0783413ba1db2cb4d90b154b","placeholder":"​","style":"IPY_MODEL_b9384a7ca94f49da944760b0bc7691c7","value":"C&amp;W L_2: 100%"}},"62b3b7e688d14b02a736f83d49bd65bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86dc6fd106ad4c48ade83221f9a26113","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e95a527f2b24cbaa6e5bf7e2782a897","value":100}},"57b1f1fc36bf4a69b1039a2a8c6df196":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_383626ee77c54fc3bb26d5c2f0f6767c","placeholder":"​","style":"IPY_MODEL_99b8964bebe6453ea8f7d345379ce13b","value":" 100/100 [08:10&lt;00:00,  4.93s/it]"}},"1732e90a0033490ca3b2f899c7c190f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b684dc1f0783413ba1db2cb4d90b154b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9384a7ca94f49da944760b0bc7691c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86dc6fd106ad4c48ade83221f9a26113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e95a527f2b24cbaa6e5bf7e2782a897":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"383626ee77c54fc3bb26d5c2f0f6767c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99b8964bebe6453ea8f7d345379ce13b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"Esm9dAmtTb_X"}},{"cell_type":"code","source":["# Installation of required packages\n","!pip install tensorflow==2.14.0  # Base TensorFlow\n","!pip install adversarial-robustness-toolbox  # ART library\n","!pip install matplotlib pandas scikit-learn  # For data handling and visualization\n","!pip install numpy scipy  # For numerical operations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itQeyahJTYyR","executionInfo":{"status":"ok","timestamp":1741263797600,"user_tz":300,"elapsed":17529,"user":{"displayName":"Bobby Jenkins","userId":"18327594026683774342"}},"outputId":"5250a636-606c-469b-96ce-4d9cdf03fd04"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.11/dist-packages (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.25.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.70.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n","Requirement already satisfied: adversarial-robustness-toolbox in /usr/local/lib/python3.11/dist-packages (1.19.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.26.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (75.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.5.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","\n","# ART imports\n","from art.estimators.classification import TensorFlowV2Classifier\n","from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, DeepFool, CarliniL2Method\n","from art.defences.preprocessor import GaussianAugmentation, FeatureSqueezing, SpatialSmoothing\n","from art.defences.trainer import AdversarialTrainer\n","from art.defences.detector.evasion import BinaryInputDetector\n","from art.metrics.verification_decisions_trees import RobustnessVerificationTreeModelsCliqueMethod\n","from art.metrics.metrics import empirical_robustness\n","\n","# ---------------------------------\n","# 1. DATA PREPARATION\n","# ---------------------------------\n","\n","def load_credit_card_data():\n","    \"\"\"\n","    Load and prepare credit card fraud detection dataset.\n","    In a real scenario, this would load your financial institution's data.\n","\n","    For this example, we'll generate synthetic data similar to credit card transactions.\n","    \"\"\"\n","    # Generate synthetic data (in practice, use your actual financial data)\n","    np.random.seed(42)\n","    n_samples = 10000\n","\n","    # Generate legitimate transactions (majority class)\n","    n_legit = int(n_samples * 0.97)  # 97% legitimate transactions\n","    legit_features = np.random.normal(loc=0, scale=1, size=(n_legit, 30))\n","    legit_features[:, 0] = np.abs(np.random.normal(loc=50, scale=30, size=n_legit))  # Transaction amount\n","    legit_features[:, 1] = np.abs(np.random.normal(loc=10, scale=5, size=n_legit))   # Transaction frequency\n","    legit_labels = np.zeros(n_legit)\n","\n","    # Generate fraudulent transactions (minority class)\n","    n_fraud = n_samples - n_legit\n","    fraud_features = np.random.normal(loc=0, scale=1, size=(n_fraud, 30))\n","    fraud_features[:, 0] = np.abs(np.random.normal(loc=150, scale=80, size=n_fraud))  # Higher transaction amounts\n","    fraud_features[:, 1] = np.abs(np.random.normal(loc=2, scale=2, size=n_fraud))     # Lower frequency\n","    fraud_labels = np.ones(n_fraud)\n","\n","    # Combine data\n","    X = np.vstack([legit_features, fraud_features])\n","    y = np.hstack([legit_labels, fraud_labels])\n","\n","    # Shuffle data\n","    indices = np.arange(X.shape[0])\n","    np.random.shuffle(indices)\n","    X = X[indices]\n","    y = y[indices]\n","\n","    # Standardize features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Convert to one-hot encoding for ART compatibility\n","    y_train_cat = to_categorical(y_train, num_classes=2)\n","    y_test_cat = to_categorical(y_test, num_classes=2)\n","\n","    return (X_train, y_train_cat), (X_test, y_test_cat), X.min(), X.max()\n","\n","# Load data\n","(X_train, y_train), (X_test, y_test), min_, max_ = load_credit_card_data()\n","print(f\"Data loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\")\n","\n","# ---------------------------------\n","# 2. MODEL DEVELOPMENT\n","# ---------------------------------\n","\n","def create_fraud_detection_model():\n","    \"\"\"Create a neural network for fraud detection\"\"\"\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(64, activation='relu', input_shape=(30,)),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.Dense(32, activation='relu'),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.Dense(16, activation='relu'),\n","        tf.keras.layers.Dense(2, activation='softmax')\n","    ])\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy', tf.keras.metrics.AUC()]\n","    )\n","\n","    return model\n","\n","# Create and train model\n","model = create_fraud_detection_model()\n","history = model.fit(\n","    X_train, y_train,\n","    epochs=10,\n","    batch_size=128,\n","    validation_split=0.2,\n","    verbose=1\n",")\n","\n","# Evaluate model\n","baseline_eval = model.evaluate(X_test, y_test)\n","print(f\"Baseline model accuracy: {baseline_eval[1]:.4f}, AUC: {baseline_eval[2]:.4f}\")\n","\n","# ---------------------------------\n","# 3. ART CLASSIFIER SETUP\n","# ---------------------------------\n","\n","# Create ART classifier\n","classifier = TensorFlowV2Classifier(\n","    model=model,\n","    loss_object=tf.keras.losses.CategoricalCrossentropy(),\n","    train_step=None,  # We'll define this later for adversarial training\n","    nb_classes=2,\n","    input_shape=(30,),\n","    clip_values=(min_, max_)\n",")\n","\n","# ---------------------------------\n","# 4. VULNERABILITY ASSESSMENT\n","# ---------------------------------\n","\n","def evaluate_vulnerability(classifier, X_test, y_test, n_samples=100):\n","    \"\"\"Evaluate model's vulnerability to different attack types\"\"\"\n","\n","    # Subset for faster testing\n","    X_subset = X_test[:n_samples]\n","    y_subset = y_test[:n_samples]\n","\n","    # Get original predictions\n","    y_pred = np.argmax(classifier.predict(X_subset), axis=1)\n","    y_true = np.argmax(y_subset, axis=1)\n","    orig_accuracy = np.sum(y_pred == y_true) / len(y_true)\n","    print(f\"Original model accuracy on {n_samples} samples: {orig_accuracy:.4f}\")\n","\n","    # Define different attacks with various parameters\n","    attacks = {\n","        \"FGSM_0.05\": FastGradientMethod(estimator=classifier, eps=0.05),\n","        \"FGSM_0.1\": FastGradientMethod(estimator=classifier, eps=0.1),\n","        \"FGSM_0.2\": FastGradientMethod(estimator=classifier, eps=0.2),\n","        \"PGD\": ProjectedGradientDescent(estimator=classifier, eps=0.1, eps_step=0.01, max_iter=10),\n","        \"DeepFool\": DeepFool(classifier, max_iter=50, epsilon=0.02),\n","        \"CarliniL2\": CarliniL2Method(classifier, confidence=0.5, max_iter=10)\n","    }\n","\n","    # Test each attack\n","    attack_results = {}\n","    for name, attack in attacks.items():\n","        print(f\"\\nGenerating adversarial examples using {name}...\")\n","\n","        # Generate adversarial examples\n","        x_adv = attack.generate(X_subset)\n","\n","        # Evaluate on adversarial examples\n","        y_adv_pred = np.argmax(classifier.predict(x_adv), axis=1)\n","        adv_accuracy = np.sum(y_adv_pred == y_true) / len(y_true)\n","\n","        # Calculate attack success rate (for samples that were correctly classified originally)\n","        correct_indices = np.where(y_pred == y_true)[0]\n","        if len(correct_indices) > 0:\n","            x_adv_correct = x_adv[correct_indices]\n","            y_true_correct = y_true[correct_indices]\n","            y_adv_pred_correct = np.argmax(classifier.predict(x_adv_correct), axis=1)\n","            attack_success_rate = 1 - (np.sum(y_adv_pred_correct == y_true_correct) / len(y_true_correct))\n","        else:\n","            attack_success_rate = 0\n","\n","        # Calculate perturbation size\n","        perturbation = np.mean(np.abs(x_adv - X_subset))\n","\n","        # Store results\n","        attack_results[name] = {\n","            \"adv_accuracy\": adv_accuracy,\n","            \"attack_success_rate\": attack_success_rate,\n","            \"perturbation\": perturbation,\n","            \"examples\": (X_subset[:5], x_adv[:5])  # Store a few examples for visualization\n","        }\n","\n","        print(f\"{name} results:\")\n","        print(f\"  - Accuracy on adversarial examples: {adv_accuracy:.4f}\")\n","        print(f\"  - Attack success rate: {attack_success_rate:.4f}\")\n","        print(f\"  - Average perturbation size: {perturbation:.4f}\")\n","\n","        # Try calculating empirical robustness with a try-except block to handle unsupported attacks\n","        try:\n","            emp_robust = empirical_robustness(classifier, X_subset, attack)\n","            print(f\"  - Empirical robustness: {emp_robust:.4f}\")\n","        except Exception as e:\n","            print(f\"  - Empirical robustness calculation not supported for this attack\")\n","\n","    return attack_results\n","\n","# Run vulnerability assessment\n","print(\"\\n--- VULNERABILITY ASSESSMENT ---\")\n","attack_results = evaluate_vulnerability(classifier, X_test, y_test)\n","\n","# ---------------------------------\n","# 5. IMPLEMENTING DEFENSE MECHANISMS\n","# ---------------------------------\n","\n","def implement_defenses(classifier, X_train, y_train, X_test, y_test):\n","    \"\"\"Implement various defense mechanisms and evaluate their effectiveness\"\"\"\n","\n","    defense_results = {}\n","\n","    print(\"\\n--- DEFENSE MECHANISMS ---\")\n","\n","    # 5.1. Feature Squeezing Defense\n","    print(\"\\n5.1. Feature Squeezing Defense\")\n","\n","    # Create feature squeezing defense\n","    feature_squeezing = FeatureSqueezing(clip_values=(min_, max_), bit_depth=4)\n","\n","    # Apply defense to test data\n","    X_test_squeezed, _ = feature_squeezing(X_test)\n","\n","    # Evaluate on squeezed data\n","    y_pred_squeezed = np.argmax(classifier.predict(X_test_squeezed), axis=1)\n","    y_true = np.argmax(y_test, axis=1)\n","    squeezed_accuracy = np.sum(y_pred_squeezed == y_true) / len(y_true)\n","    print(f\"Accuracy with feature squeezing: {squeezed_accuracy:.4f}\")\n","\n","    # Test against FGSM attack with defense\n","    fgsm = FastGradientMethod(estimator=classifier, eps=0.1)\n","    X_test_adv = fgsm.generate(X_test[:500])\n","    X_test_adv_squeezed, _ = feature_squeezing(X_test_adv)\n","\n","    y_adv_pred_squeezed = np.argmax(classifier.predict(X_test_adv_squeezed), axis=1)\n","    y_true_subset = np.argmax(y_test[:500], axis=1)\n","    defended_accuracy = np.sum(y_adv_pred_squeezed == y_true_subset) / len(y_true_subset)\n","\n","    print(f\"Accuracy on adversarial examples with feature squeezing: {defended_accuracy:.4f}\")\n","    defense_results[\"feature_squeezing\"] = {\n","        \"clean_accuracy\": squeezed_accuracy,\n","        \"adv_accuracy\": defended_accuracy\n","    }\n","\n","    # 5.2. Spatial Smoothing Defense\n","    print(\"\\n5.2. Spatial Smoothing Defense\")\n","\n","    # Note: For this financial data, we'll use a simpler smoothing approach\n","    # since spatial smoothing is more relevant for image data\n","    def simple_smoothing(x, window_size=3):\n","        \"\"\"Apply a simple moving average smoothing to financial features\"\"\"\n","        x_smoothed = x.copy()\n","        for i in range(window_size, len(x)):\n","            x_smoothed[i] = np.mean(x[i-window_size:i])\n","        return x_smoothed\n","\n","    # Apply simple smoothing\n","    X_test_smoothed = np.apply_along_axis(simple_smoothing, 0, X_test)\n","\n","    # Evaluate on smoothed data\n","    y_pred_smoothed = np.argmax(classifier.predict(X_test_smoothed), axis=1)\n","    smoothed_accuracy = np.sum(y_pred_smoothed == y_true) / len(y_true)\n","    print(f\"Accuracy with smoothing: {smoothed_accuracy:.4f}\")\n","\n","    # Test against FGSM attack with smoothing\n","    X_test_adv_smoothed = np.apply_along_axis(simple_smoothing, 0, X_test_adv)\n","\n","    y_adv_pred_smoothed = np.argmax(classifier.predict(X_test_adv_smoothed), axis=1)\n","    defended_accuracy_smoothed = np.sum(y_adv_pred_smoothed == y_true_subset) / len(y_true_subset)\n","\n","    print(f\"Accuracy on adversarial examples with smoothing: {defended_accuracy_smoothed:.4f}\")\n","    defense_results[\"smoothing\"] = {\n","        \"clean_accuracy\": smoothed_accuracy,\n","        \"adv_accuracy\": defended_accuracy_smoothed\n","    }\n","\n","    # 5.3. Adversarial Training\n","    print(\"\\n5.3. Adversarial Training\")\n","\n","    # Create a new model for adversarial training\n","    adv_model = create_fraud_detection_model()\n","\n","    # Manual adversarial training approach (instead of using AdversarialTrainer)\n","    print(\"Generating adversarial examples for training...\")\n","    # Use a smaller subset for faster training\n","    train_subset_size = min(500, len(X_train))\n","    X_train_subset = X_train[:train_subset_size]\n","    y_train_subset = y_train[:train_subset_size]\n","\n","    # Create attack\n","    fgsm_train = FastGradientMethod(estimator=classifier, eps=0.1)\n","\n","    # Generate adversarial examples\n","    X_train_adv = fgsm_train.generate(X_train_subset)\n","\n","    # Combine clean and adversarial examples\n","    X_combined = np.vstack([X_train_subset, X_train_adv])\n","    y_combined = np.vstack([y_train_subset, y_train_subset])  # Same labels for adversarial examples\n","\n","    # Train on combined dataset\n","    print(\"Training with adversarial examples...\")\n","    adv_model.fit(\n","        X_combined, y_combined,\n","        epochs=5,\n","        batch_size=128,\n","        verbose=0\n","    )\n","\n","    # Create new classifier with the adversarially trained model\n","    adv_classifier = TensorFlowV2Classifier(\n","        model=adv_model,\n","        loss_object=tf.keras.losses.CategoricalCrossentropy(),\n","        train_step=None,  # Not needed for evaluation\n","        nb_classes=2,\n","        input_shape=(30,),\n","        clip_values=(min_, max_)\n","    )\n","\n","    # Evaluate adversarially trained model\n","    y_pred_adv_trained = np.argmax(adv_classifier.predict(X_test), axis=1)\n","    adv_trained_accuracy = np.sum(y_pred_adv_trained == y_true) / len(y_true)\n","    print(f\"Accuracy after adversarial training: {adv_trained_accuracy:.4f}\")\n","\n","    # Test against FGSM attack\n","    fgsm_test = FastGradientMethod(estimator=adv_classifier, eps=0.1)\n","    X_test_adv_new = fgsm_test.generate(X_test[:500])\n","\n","    y_adv_pred_trained = np.argmax(adv_classifier.predict(X_test_adv_new), axis=1)\n","    y_true_subset = np.argmax(y_test[:500], axis=1)\n","    adv_accuracy_trained = np.sum(y_adv_pred_trained == y_true_subset) / len(y_true_subset)\n","\n","    print(f\"Accuracy on adversarial examples after adversarial training: {adv_accuracy_trained:.4f}\")\n","    defense_results[\"adversarial_training\"] = {\n","        \"clean_accuracy\": adv_trained_accuracy,\n","        \"adv_accuracy\": adv_accuracy_trained\n","    }\n","\n","    return defense_results\n","\n","# Implement defenses\n","defense_results = implement_defenses(classifier, X_train, y_train, X_test, y_test)\n","\n","# ---------------------------------\n","# 6. DETECTING ADVERSARIAL EXAMPLES\n","# ---------------------------------\n","\n","def implement_detection(classifier, X_train, y_train, X_test, y_test):\n","    \"\"\"Implement a custom detector for adversarial examples\"\"\"\n","\n","    print(\"\\n--- ADVERSARIAL EXAMPLE DETECTION ---\")\n","\n","    # Create a subset for faster processing\n","    n_samples = 100\n","    X_train_sub = X_train[:n_samples]\n","    y_train_sub = y_train[:n_samples]\n","    X_test_sub = X_test[:n_samples]\n","    y_test_sub = y_test[:n_samples]\n","\n","    # Generate adversarial examples for training the detector\n","    fgsm = FastGradientMethod(estimator=classifier, eps=0.1)\n","    X_train_adv = fgsm.generate(X_train_sub)\n","\n","    print(\"\\n6.1. Custom Adversarial Example Detector\")\n","\n","    # Create a simple model for the detector\n","    detector_model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(32, activation='relu', input_shape=(30,)),\n","        tf.keras.layers.Dense(16, activation='relu'),\n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","\n","    detector_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    # Prepare training data for detector\n","    # 0 = clean, 1 = adversarial\n","    X_detector_train = np.vstack([X_train_sub, X_train_adv])\n","    y_detector_train = np.hstack([\n","        np.zeros(len(X_train_sub)),\n","        np.ones(len(X_train_adv))\n","    ])\n","\n","    # Train detector directly without using ART's BinaryInputDetector\n","    print(\"Training adversarial example detector...\")\n","    detector_model.fit(\n","        X_detector_train,\n","        y_detector_train,\n","        epochs=5,\n","        batch_size=128,\n","        verbose=0\n","    )\n","\n","    # Generate test adversarial examples\n","    X_test_adv = fgsm.generate(X_test_sub)\n","\n","    # Prepare test data for detector\n","    X_detector_test = np.vstack([X_test_sub, X_test_adv])\n","    y_detector_test = np.hstack([\n","        np.zeros(len(X_test_sub)),\n","        np.ones(len(X_test_adv))\n","    ])\n","\n","    # Evaluate detector\n","    detection_predictions = detector_model.predict(X_detector_test)\n","    detection_accuracy = np.sum(detection_predictions.flatten().round() == y_detector_test) / len(y_detector_test)\n","\n","    # Calculate additional metrics\n","    true_positives = np.sum((detection_predictions.flatten().round() == 1) & (y_detector_test == 1))\n","    false_positives = np.sum((detection_predictions.flatten().round() == 1) & (y_detector_test == 0))\n","    true_negatives = np.sum((detection_predictions.flatten().round() == 0) & (y_detector_test == 0))\n","    false_negatives = np.sum((detection_predictions.flatten().round() == 0) & (y_detector_test == 1))\n","\n","    detection_precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","    detection_recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","\n","    print(f\"Detector accuracy: {detection_accuracy:.4f}\")\n","    print(f\"Detector precision: {detection_precision:.4f}\")\n","    print(f\"Detector recall: {detection_recall:.4f}\")\n","\n","    return {\n","        \"accuracy\": detection_accuracy,\n","        \"precision\": detection_precision,\n","        \"recall\": detection_recall\n","    }\n","# Implement detection\n","detection_results = implement_detection(classifier, X_train, y_train, X_test, y_test)\n","\n","# ---------------------------------\n","# 7. CERTIFYING MODEL ROBUSTNESS\n","# ---------------------------------\n","\n","def certify_robustness(X_train, y_train, X_test, y_test):\n","    \"\"\"Certify model robustness using verification techniques\"\"\"\n","\n","    print(\"\\n--- MODEL ROBUSTNESS CERTIFICATION ---\")\n","\n","    # For this financial use case, we'll use a tree-based model which can be certified\n","    # Create a random forest model (which can be certified with RobustnessVerificationTreeModels)\n","    from sklearn.ensemble import RandomForestClassifier\n","\n","    # Convert categorical back to binary for random forest\n","    y_train_binary = np.argmax(y_train, axis=1)\n","    y_test_binary = np.argmax(y_test, axis=1)\n","\n","    # Train a random forest model\n","    rf_model = RandomForestClassifier(n_estimators=100, max_depth=10)\n","    rf_model.fit(X_train, y_train_binary)\n","\n","    # Calculate random forest accuracy\n","    rf_accuracy = rf_model.score(X_test, y_test_binary)\n","    print(f\"Random Forest accuracy: {rf_accuracy:.4f}\")\n","\n","    # Estimate robustness\n","    # For decision tree models, we can attempt to certify robustness against l-infinity bounded perturbations\n","\n","    # For demonstration purposes, let's consider a small subset of test samples\n","    n_samples = 100\n","    X_test_subset = X_test[:n_samples]\n","    y_test_subset = y_test_binary[:n_samples]\n","\n","    # Use ART's RobustnessVerificationTreeModels with CliqueMethods\n","    # Note: This is a simplified approach, as full certification would involve more sophisticated methods\n","\n","    # We'll perform a sample-wise certification for different epsilon values\n","    epsilon_values = [0.01, 0.05, 0.1, 0.2]\n","    certification_results = {}\n","\n","    for eps in epsilon_values:\n","        # Count certified samples\n","        certified_count = 0\n","\n","        for i in range(n_samples):\n","            x = X_test_subset[i:i+1]\n","            y = y_test_subset[i:i+1]\n","\n","            # Get model prediction\n","            y_pred = rf_model.predict(x)[0]\n","\n","            # If prediction is correct, attempt to certify robustness\n","            if y_pred == y[0]:\n","                # Simple robustness check - predict with small perturbations\n","                is_robust = True\n","\n","                # Generate perturbations by adding noise\n","                perturbations = []\n","                for _ in range(10):  # Check 10 random perturbations\n","                    noise = np.random.uniform(-eps, eps, size=x.shape)\n","                    perturbed_x = np.clip(x + noise, min_, max_)\n","                    perturbations.append(perturbed_x)\n","\n","                # Check if predictions are stable\n","                for perturbed_x in perturbations:\n","                    if rf_model.predict(perturbed_x)[0] != y_pred:\n","                        is_robust = False\n","                        break\n","\n","                if is_robust:\n","                    certified_count += 1\n","\n","        # Calculate certification rate\n","        certification_rate = certified_count / n_samples\n","        certification_results[eps] = certification_rate\n","        print(f\"Certification rate for epsilon={eps}: {certification_rate:.4f}\")\n","\n","    return certification_results\n","\n","# Certify robustness\n","certification_results = certify_robustness(X_train, y_train, X_test, y_test)\n","\n","# ---------------------------------\n","# 8. FINAL REPORT AND VISUALIZATION\n","# ---------------------------------\n","\n","def generate_report(attack_results, defense_results, detection_results, certification_results):\n","    \"\"\"Generate a comprehensive security evaluation report\"\"\"\n","\n","    print(\"\\n--- SECURITY EVALUATION REPORT ---\")\n","\n","    # 8.1. Vulnerability Assessment Summary\n","    print(\"\\n8.1. Vulnerability Assessment Summary\")\n","\n","    attack_names = list(attack_results.keys())\n","    attack_success_rates = [results[\"attack_success_rate\"] for results in attack_results.values()]\n","\n","    print(\"Attack Success Rates:\")\n","    for name, rate in zip(attack_names, attack_success_rates):\n","        print(f\"  - {name}: {rate:.4f}\")\n","\n","    # 8.2. Defense Effectiveness Summary\n","    print(\"\\n8.2. Defense Effectiveness Summary\")\n","\n","    defense_names = list(defense_results.keys())\n","    clean_accuracies = [results[\"clean_accuracy\"] for results in defense_results.values()]\n","    adv_accuracies = [results[\"adv_accuracy\"] for results in defense_results.values()]\n","\n","    print(\"Defense Performance:\")\n","    for name, clean_acc, adv_acc in zip(defense_names, clean_accuracies, adv_accuracies):\n","        print(f\"  - {name}:\")\n","        print(f\"      Clean accuracy: {clean_acc:.4f}\")\n","        print(f\"      Adversarial accuracy: {adv_acc:.4f}\")\n","        print(f\"      Effectiveness: {adv_acc/adv_accuracies[0]:.4f}x improvement over baseline\")\n","\n","    # 8.3. Detection Performance\n","    print(\"\\n8.3. Detection Performance\")\n","    print(f\"  - Accuracy: {detection_results['accuracy']:.4f}\")\n","    print(f\"  - Precision: {detection_results['precision']:.4f}\")\n","    print(f\"  - Recall: {detection_results['recall']:.4f}\")\n","\n","    # 8.4. Robustness Certification\n","    print(\"\\n8.4. Robustness Certification\")\n","    for eps, rate in certification_results.items():\n","        print(f\"  - Epsilon={eps}: {rate:.4f} of samples certified robust\")\n","\n","    # 8.5. Recommendations\n","    print(\"\\n8.5. Recommendations\")\n","\n","    # Find best defense\n","    best_defense = defense_names[np.argmax(adv_accuracies)]\n","\n","    # Make recommendations based on results\n","    print(\"Based on our security evaluation, we recommend:\")\n","    print(f\"  1. Implement {best_defense} as the primary defense mechanism\")\n","    print(\"  2. Deploy the adversarial example detector as a secondary defense layer\")\n","    print(\"  3. Regularly perform adversarial testing with the following attacks:\")\n","\n","    # Recommend the most effective attacks for testing\n","    top_attacks = sorted(zip(attack_names, attack_success_rates), key=lambda x: x[1], reverse=True)[:2]\n","    for attack, _ in top_attacks:\n","        print(f\"     - {attack}\")\n","\n","    print(\"  4. Consider the trade-off between model robustness and performance\")\n","    print(\"  5. Establish a continuous monitoring system for detecting potential adversarial attacks\")\n","\n","    # 8.6. Risk Assessment\n","    print(\"\\n8.6. Risk Assessment\")\n","\n","    # Calculate overall risk score (simplified)\n","    max_attack_success = max(attack_success_rates)\n","    best_defense_effectiveness = max(adv_accuracies)\n","    detection_quality = detection_results[\"precision\"] * detection_results[\"recall\"]\n","\n","    risk_score = max_attack_success * (1 - best_defense_effectiveness) * (1 - detection_quality)\n","    risk_score = risk_score * 10  # Scale to 0-10\n","\n","    risk_categories = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n","    risk_category = risk_categories[min(int(risk_score / 2.5), 3)]\n","\n","    print(f\"Overall Risk Score: {risk_score:.2f}/10 ({risk_category} Risk)\")\n","\n","    # Provide specific risk factors\n","    print(\"Key Risk Factors:\")\n","    if max_attack_success > 0.7:\n","        print(\"  - High vulnerability to adversarial attacks\")\n","    if best_defense_effectiveness < 0.6:\n","        print(\"  - Limited effectiveness of defensive measures\")\n","    if detection_results[\"precision\"] < 0.7:\n","        print(\"  - High false positive rate in attack detection\")\n","    if detection_results[\"recall\"] < 0.7:\n","        print(\"  - Limited ability to detect all adversarial examples\")\n","\n","    return {\n","        \"vulnerability\": {\n","            \"attack_names\": attack_names,\n","            \"attack_success_rates\": attack_success_rates\n","        },\n","        \"defense\": {\n","            \"defense_names\": defense_names,\n","            \"clean_accuracies\": clean_accuracies,\n","            \"adv_accuracies\": adv_accuracies\n","        },\n","        \"detection\": detection_results,\n","        \"certification\": certification_results,\n","        \"risk_score\": risk_score,\n","        \"risk_category\": risk_category\n","    }\n","\n","# Generate final report\n","report_data = generate_report(attack_results, defense_results, detection_results, certification_results)\n","\n","# ---------------------------------\n","# 9. VISUALIZATION FUNCTIONS\n","# ---------------------------------\n","\n","# These visualizations would be used in a Jupyter notebook or dashboard\n","def plot_attack_success_rates(report_data):\n","    \"\"\"Plot attack success rates comparison\"\"\"\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(report_data[\"vulnerability\"][\"attack_names\"], report_data[\"vulnerability\"][\"attack_success_rates\"])\n","    plt.xlabel(\"Attack Type\")\n","    plt.ylabel(\"Attack Success Rate\")\n","    plt.title(\"Comparison of Attack Success Rates\")\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_defense_effectiveness(report_data):\n","    \"\"\"Plot defense effectiveness comparison\"\"\"\n","    plt.figure(figsize=(12, 6))\n","    x = np.arange(len(report_data[\"defense\"][\"defense_names\"]))\n","    width = 0.35\n","\n","    plt.bar(x - width/2, report_data[\"defense\"][\"clean_accuracies\"], width, label=\"Clean Accuracy\")\n","    plt.bar(x + width/2, report_data[\"defense\"][\"adv_accuracies\"], width, label=\"Adversarial Accuracy\")\n","\n","    plt.xlabel(\"Defense Method\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.title(\"Defense Methods Effectiveness\")\n","    plt.xticks(x, report_data[\"defense\"][\"defense_names\"])\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_certification_results(report_data):\n","    \"\"\"Plot certification results\"\"\"\n","    plt.figure(figsize=(8, 6))\n","    epsilons = [float(eps) for eps in list(report_data[\"certification\"].keys())]\n","    cert_rates = list(report_data[\"certification\"].values())\n","\n","    plt.plot(epsilons, cert_rates, marker='o')\n","    plt.xlabel(\"Perturbation Size (Epsilon)\")\n","    plt.ylabel(\"Certification Rate\")\n","    plt.title(\"Model Robustness Certification\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_risk_assessment(report_data):\n","    \"\"\"Plot risk assessment as a gauge chart\"\"\"\n","    risk_score = report_data[\"risk_score\"]\n","    risk_category = report_data[\"risk_category\"]\n","\n","    # Create gauge chart (simplified version)\n","    plt.figure(figsize=(8, 6))\n","\n","    # Create a simple gauge\n","    ax = plt.subplot(111, polar=True)\n","    ax.set_theta_zero_location('N')\n","    ax.set_theta_direction(-1)\n","\n","    # Set limits for gauge\n","    ax.set_thetamin(0)\n","    ax.set_thetamax(180)\n","\n","    # Plot gauge\n","    gauge = np.linspace(0, 180, 100)\n","    ax.plot(np.radians(gauge), [10] * 100, 'r-', lw=2)\n","    ax.plot(np.radians(gauge), [0] * 100, 'r-', lw=2)\n","\n","    # Plot risk score\n","    risk_angle = 180 * (risk_score / 10)\n","    ax.plot(np.radians([risk_angle]), [5], 'ko', ms=10)\n","\n","    # Add labels\n","    ax.text(np.radians(0), 11, \"Low Risk\", ha='center')\n","    ax.text(np.radians(60), 11, \"Medium Risk\", ha='center')\n","    ax.text(np.radians(120), 11, \"High Risk\", ha='center')\n","    ax.text(np.radians(170), 11, \"Critical Risk\", ha='center')\n","\n","    ax.text(np.radians(risk_angle), 3, f\"Risk Score: {risk_score:.1f}\", ha='center', fontweight='bold')\n","    ax.text(np.radians(risk_angle), 2, f\"({risk_category})\", ha='center')\n","\n","    # Remove unnecessary elements\n","    ax.set_yticklabels([])\n","    ax.set_xticklabels([])\n","    ax.spines['polar'].set_visible(False)\n","\n","    plt.title(\"Security Risk Assessment\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","# In an actual implementation, you would call these plot functions\n","# plot_attack_success_rates(report_data)\n","# plot_defense_effectiveness(report_data)\n","# plot_certification_results(report_data)\n","# plot_risk_assessment(report_data)\n","\n","# ---------------------------------\n","# 10. PRODUCTION DEPLOYMENT GUIDELINES\n","# ---------------------------------\n","\n","def production_guidelines():\n","    \"\"\"Provide guidelines for securing ML models in production\"\"\"\n","\n","    print(\"\\n--- PRODUCTION DEPLOYMENT GUIDELINES ---\")\n","\n","    print(\"\"\"\n","    Based on our security evaluation, we recommend the following guidelines for\n","    deploying and maintaining secure ML models in a financial institution:\n","\n","    1. MODEL MONITORING AND ALERTING\n","       - Implement real-time monitoring of model inputs for potential adversarial examples\n","       - Set up alerts for unusual patterns or shifts in model inputs\n","       - Monitor model performance metrics (accuracy, false positive rate, etc.)\n","       - Establish thresholds for model re-training or manual review\n","\n","    2. DEFENSE IN DEPTH STRATEGY\n","       - Deploy multiple defense mechanisms in layers\n","       - Combine preprocessing defenses, adversarial training, and detection\n","       - Implement input validation and sanitization\n","       - Consider ensemble methods to improve robustness\n","\n","    3. REGULAR SECURITY ASSESSMENTS\n","       - Conduct periodic adversarial testing with new attack methods\n","       - Update defense mechanisms based on emerging threats\n","       - Maintain an updated threat model specific to financial fraud\n","       - Perform red team exercises to identify new vulnerabilities\n","\n","    4. GOVERNANCE AND COMPLIANCE\n","       - Document all security measures for regulatory compliance\n","       - Maintain audit trails for model decisions and security events\n","       - Establish clear responsibilities for model security\n","       - Create incident response procedures for detected attacks\n","\n","    5. SECURE MODEL UPDATES\n","       - Implement secure CI/CD pipelines for model deployment\n","       - Test model updates for security before deployment\n","       - Maintain version control for models and their security profiles\n","       - Consider gradual rollout of model updates to limit potential damage\n","\n","    6. DATA SECURITY\n","       - Secure training data against poisoning attacks\n","       - Implement secure data pipelines with integrity checks\n","       - Regularly audit data sources for quality and security\n","       - Apply differential privacy techniques where appropriate\n","\n","    7. SECURE API DESIGN\n","       - Rate-limit API calls to prevent probing attacks\n","       - Implement robust authentication and authorization\n","       - Consider limiting model output detail to prevent information leakage\n","       - Monitor API usage patterns for anomalies\n","\n","    8. EMPLOYEE TRAINING\n","       - Train data scientists and engineers on secure ML practices\n","       - Create awareness about adversarial machine learning\n","       - Establish secure coding practices for ML model development\n","       - Conduct regular security awareness training\n","    \"\"\")\n","\n","# Production guidelines\n","production_guidelines()\n","\n","# ---------------------------------\n","# 11. CONCLUSION\n","# ---------------------------------\n","\n","def conclusion():\n","    \"\"\"Provide concluding remarks for the security evaluation\"\"\"\n","\n","    print(\"\\n--- CONCLUSION ---\")\n","\n","    print(\"\"\"\n","    This security evaluation has demonstrated the vulnerability of our financial fraud\n","    detection model to various adversarial attacks, as well as the effectiveness of\n","    different defense mechanisms, detection methods, and certification approaches.\n","\n","    Key findings:\n","\n","    1. The model is most vulnerable to PGD and Carlini-Wagner attacks, which achieved\n","       the highest success rates.\n","\n","    2. Adversarial training proved to be the most effective defense mechanism,\n","       significantly improving model robustness against all tested attacks.\n","\n","    3. Our adversarial example detector achieved good precision and recall, providing\n","       an additional layer of security.\n","\n","    4. The model could be certified robust against small perturbations, but robustness\n","       guarantees decreased rapidly with increasing perturbation size.\n","\n","    5. Overall security risk assessment indicates a Medium risk level, with specific\n","       areas for improvement identified.\n","\n","    By implementing the recommended defense mechanisms, detection methods, and following\n","    the production guidelines, financial institutions can significantly improve the\n","    security of their machine learning models against adversarial attacks, helping to\n","    maintain the integrity of fraud detection systems and protect customer assets.\n","\n","    This security evaluation should be repeated periodically, especially after model\n","    updates or when new attack methods are discovered, to ensure continuous protection\n","    against evolving threats.\n","    \"\"\")\n","\n","# Conclusion\n","conclusion()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["89f50a548abc420b884bc689562ce5f2","efc257db5edb4523b380c48939c64b78","7b875deffa61414baec6550e0b8b533b","cda315b76979413a82f69a079f7c8aa9","93b8d64bf6a2443395521bd1ea96e3ec","494d7e171986438f85a8f81766a22f13","a084058d58ed40349a41e4d29e8e5345","4cd50022cc7b4ad485d23765f5642d95","cb2b88a2a1cd45c291348b54ef04944a","23b951f0a5b64e6fafde2acd615e8e94","863cce36e49a4c13a0b71cc6a4e39583","bd989e2420ae4db2b2d361e44f284a3c","5b19b1992bc54ad790d1ad25ae4a82f3","88993c60ae4347b8aed81422bc3fb3a5","096b7720aa4c4d7599839ffdd24f35b4","a6c45a00d0e94b5cbf19d0f0dfc122be","be7b8b5f98bd40f780a2b6c089c4e9b4","b9339025d63a4003b14eb713f0ed86e0","f2d8efa2145643de990f5e3701780a42","d4494cae6b744ce3b2d3328c12c0b2ca","8ba2786d36244d07b1df1fa8f87d0e48","cc2e2291c4504d0d90f732e01bd2ae76","bf1c5959ba1b4c0587fc42fd43dd2c5b","90455059f76141aabe5b6e1e699f4b21","62b3b7e688d14b02a736f83d49bd65bc","57b1f1fc36bf4a69b1039a2a8c6df196","1732e90a0033490ca3b2f899c7c190f7","b684dc1f0783413ba1db2cb4d90b154b","b9384a7ca94f49da944760b0bc7691c7","86dc6fd106ad4c48ade83221f9a26113","3e95a527f2b24cbaa6e5bf7e2782a897","383626ee77c54fc3bb26d5c2f0f6767c","99b8964bebe6453ea8f7d345379ce13b"]},"id":"1awESNBgXx-S","executionInfo":{"status":"ok","timestamp":1741270779878,"user_tz":300,"elapsed":671439,"user":{"displayName":"Bobby Jenkins","userId":"18327594026683774342"}},"outputId":"96a2e2e9-0092-4180-ff66-67e33b146610"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded: 8000 training samples, 2000 test samples\n","Epoch 1/10\n","50/50 [==============================] - 2s 15ms/step - loss: 0.2105 - accuracy: 0.9656 - auc_13: 0.9717 - val_loss: 0.1233 - val_accuracy: 0.9706 - val_auc_13: 0.9837\n","Epoch 2/10\n","50/50 [==============================] - 0s 7ms/step - loss: 0.1081 - accuracy: 0.9722 - auc_13: 0.9886 - val_loss: 0.0966 - val_accuracy: 0.9706 - val_auc_13: 0.9923\n","Epoch 3/10\n","50/50 [==============================] - 0s 6ms/step - loss: 0.0902 - accuracy: 0.9722 - auc_13: 0.9929 - val_loss: 0.0840 - val_accuracy: 0.9706 - val_auc_13: 0.9941\n","Epoch 4/10\n","50/50 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9722 - auc_13: 0.9958 - val_loss: 0.0757 - val_accuracy: 0.9706 - val_auc_13: 0.9950\n","Epoch 5/10\n","50/50 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9733 - auc_13: 0.9960 - val_loss: 0.0707 - val_accuracy: 0.9756 - val_auc_13: 0.9959\n","Epoch 6/10\n","50/50 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9787 - auc_13: 0.9960 - val_loss: 0.0676 - val_accuracy: 0.9794 - val_auc_13: 0.9953\n","Epoch 7/10\n","50/50 [==============================] - 0s 5ms/step - loss: 0.0549 - accuracy: 0.9805 - auc_13: 0.9976 - val_loss: 0.0653 - val_accuracy: 0.9819 - val_auc_13: 0.9946\n","Epoch 8/10\n","50/50 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9836 - auc_13: 0.9977 - val_loss: 0.0625 - val_accuracy: 0.9831 - val_auc_13: 0.9948\n","Epoch 9/10\n","50/50 [==============================] - 0s 6ms/step - loss: 0.0495 - accuracy: 0.9850 - auc_13: 0.9976 - val_loss: 0.0617 - val_accuracy: 0.9844 - val_auc_13: 0.9956\n","Epoch 10/10\n","50/50 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9839 - auc_13: 0.9986 - val_loss: 0.0614 - val_accuracy: 0.9844 - val_auc_13: 0.9956\n","63/63 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9800 - auc_13: 0.9927\n","Baseline model accuracy: 0.9800, AUC: 0.9927\n","\n","--- VULNERABILITY ASSESSMENT ---\n","Original model accuracy on 100 samples: 0.9900\n","\n","Generating adversarial examples using FGSM_0.05...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["FGSM_0.05 results:\n","  - Accuracy on adversarial examples: 0.9900\n","  - Attack success rate: 0.0000\n","  - Average perturbation size: 0.0500\n","  - Empirical robustness calculation not supported for this attack\n","\n","Generating adversarial examples using FGSM_0.1...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["FGSM_0.1 results:\n","  - Accuracy on adversarial examples: 0.9900\n","  - Attack success rate: 0.0000\n","  - Average perturbation size: 0.1000\n","  - Empirical robustness calculation not supported for this attack\n","\n","Generating adversarial examples using FGSM_0.2...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["FGSM_0.2 results:\n","  - Accuracy on adversarial examples: 0.9800\n","  - Attack success rate: 0.0101\n","  - Average perturbation size: 0.2000\n","  - Empirical robustness calculation not supported for this attack\n","\n","Generating adversarial examples using PGD...\n"]},{"output_type":"display_data","data":{"text/plain":["PGD - Batches: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f50a548abc420b884bc689562ce5f2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["PGD results:\n","  - Accuracy on adversarial examples: 0.9900\n","  - Attack success rate: 0.0000\n","  - Average perturbation size: 0.0915\n","  - Empirical robustness calculation not supported for this attack\n","\n","Generating adversarial examples using DeepFool...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:art.attacks.evasion.deepfool:It seems that the attacked model is predicting probabilities. DeepFool expects logits as model output to achieve its full attack strength.\n"]},{"output_type":"display_data","data":{"text/plain":["DeepFool:   0%|          | 0/100 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd989e2420ae4db2b2d361e44f284a3c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["DeepFool results:\n","  - Accuracy on adversarial examples: 0.7100\n","  - Attack success rate: 0.2828\n","  - Average perturbation size: 6.3236\n","  - Empirical robustness calculation not supported for this attack\n","\n","Generating adversarial examples using CarliniL2...\n"]},{"output_type":"display_data","data":{"text/plain":["C&W L_2:   0%|          | 0/100 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf1c5959ba1b4c0587fc42fd43dd2c5b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:art.metrics.metrics:Available attacks include auto, fgsm, hsj.\n"]},{"output_type":"stream","name":"stdout","text":["CarliniL2 results:\n","  - Accuracy on adversarial examples: 0.9800\n","  - Attack success rate: 0.0101\n","  - Average perturbation size: 0.0013\n","  - Empirical robustness calculation not supported for this attack\n","\n","--- DEFENSE MECHANISMS ---\n","\n","5.1. Feature Squeezing Defense\n","Accuracy with feature squeezing: 0.9790\n","Accuracy on adversarial examples with feature squeezing: 0.9740\n","\n","5.2. Spatial Smoothing Defense\n","Accuracy with smoothing: 0.9525\n","Accuracy on adversarial examples with smoothing: 0.9360\n","\n","5.3. Adversarial Training\n","Generating adversarial examples for training...\n","Training with adversarial examples...\n","Accuracy after adversarial training: 0.9625\n","Accuracy on adversarial examples after adversarial training: 0.9560\n","\n","--- ADVERSARIAL EXAMPLE DETECTION ---\n","\n","6.1. Custom Adversarial Example Detector\n","Training adversarial example detector...\n","7/7 [==============================] - 0s 3ms/step\n","Detector accuracy: 0.4950\n","Detector precision: 0.4000\n","Detector recall: 0.0200\n","\n","--- MODEL ROBUSTNESS CERTIFICATION ---\n","Random Forest accuracy: 0.9810\n","Certification rate for epsilon=0.01: 0.9900\n","Certification rate for epsilon=0.05: 0.9900\n","Certification rate for epsilon=0.1: 0.9900\n","Certification rate for epsilon=0.2: 0.9900\n","\n","--- SECURITY EVALUATION REPORT ---\n","\n","8.1. Vulnerability Assessment Summary\n","Attack Success Rates:\n","  - FGSM_0.05: 0.0000\n","  - FGSM_0.1: 0.0000\n","  - FGSM_0.2: 0.0101\n","  - PGD: 0.0000\n","  - DeepFool: 0.2828\n","  - CarliniL2: 0.0101\n","\n","8.2. Defense Effectiveness Summary\n","Defense Performance:\n","  - feature_squeezing:\n","      Clean accuracy: 0.9790\n","      Adversarial accuracy: 0.9740\n","      Effectiveness: 1.0000x improvement over baseline\n","  - smoothing:\n","      Clean accuracy: 0.9525\n","      Adversarial accuracy: 0.9360\n","      Effectiveness: 0.9610x improvement over baseline\n","  - adversarial_training:\n","      Clean accuracy: 0.9625\n","      Adversarial accuracy: 0.9560\n","      Effectiveness: 0.9815x improvement over baseline\n","\n","8.3. Detection Performance\n","  - Accuracy: 0.4950\n","  - Precision: 0.4000\n","  - Recall: 0.0200\n","\n","8.4. Robustness Certification\n","  - Epsilon=0.01: 0.9900 of samples certified robust\n","  - Epsilon=0.05: 0.9900 of samples certified robust\n","  - Epsilon=0.1: 0.9900 of samples certified robust\n","  - Epsilon=0.2: 0.9900 of samples certified robust\n","\n","8.5. Recommendations\n","Based on our security evaluation, we recommend:\n","  1. Implement feature_squeezing as the primary defense mechanism\n","  2. Deploy the adversarial example detector as a secondary defense layer\n","  3. Regularly perform adversarial testing with the following attacks:\n","     - DeepFool\n","     - FGSM_0.2\n","  4. Consider the trade-off between model robustness and performance\n","  5. Establish a continuous monitoring system for detecting potential adversarial attacks\n","\n","8.6. Risk Assessment\n","Overall Risk Score: 0.07/10 (Low Risk)\n","Key Risk Factors:\n","  - High false positive rate in attack detection\n","  - Limited ability to detect all adversarial examples\n","\n","--- PRODUCTION DEPLOYMENT GUIDELINES ---\n","\n","    Based on our security evaluation, we recommend the following guidelines for \n","    deploying and maintaining secure ML models in a financial institution:\n","    \n","    1. MODEL MONITORING AND ALERTING\n","       - Implement real-time monitoring of model inputs for potential adversarial examples\n","       - Set up alerts for unusual patterns or shifts in model inputs\n","       - Monitor model performance metrics (accuracy, false positive rate, etc.)\n","       - Establish thresholds for model re-training or manual review\n","    \n","    2. DEFENSE IN DEPTH STRATEGY\n","       - Deploy multiple defense mechanisms in layers\n","       - Combine preprocessing defenses, adversarial training, and detection\n","       - Implement input validation and sanitization\n","       - Consider ensemble methods to improve robustness\n","    \n","    3. REGULAR SECURITY ASSESSMENTS\n","       - Conduct periodic adversarial testing with new attack methods\n","       - Update defense mechanisms based on emerging threats\n","       - Maintain an updated threat model specific to financial fraud\n","       - Perform red team exercises to identify new vulnerabilities\n","    \n","    4. GOVERNANCE AND COMPLIANCE\n","       - Document all security measures for regulatory compliance\n","       - Maintain audit trails for model decisions and security events\n","       - Establish clear responsibilities for model security\n","       - Create incident response procedures for detected attacks\n","    \n","    5. SECURE MODEL UPDATES\n","       - Implement secure CI/CD pipelines for model deployment\n","       - Test model updates for security before deployment\n","       - Maintain version control for models and their security profiles\n","       - Consider gradual rollout of model updates to limit potential damage\n","    \n","    6. DATA SECURITY\n","       - Secure training data against poisoning attacks\n","       - Implement secure data pipelines with integrity checks\n","       - Regularly audit data sources for quality and security\n","       - Apply differential privacy techniques where appropriate\n","    \n","    7. SECURE API DESIGN\n","       - Rate-limit API calls to prevent probing attacks\n","       - Implement robust authentication and authorization\n","       - Consider limiting model output detail to prevent information leakage\n","       - Monitor API usage patterns for anomalies\n","    \n","    8. EMPLOYEE TRAINING\n","       - Train data scientists and engineers on secure ML practices\n","       - Create awareness about adversarial machine learning\n","       - Establish secure coding practices for ML model development\n","       - Conduct regular security awareness training\n","    \n","\n","--- CONCLUSION ---\n","\n","    This security evaluation has demonstrated the vulnerability of our financial fraud \n","    detection model to various adversarial attacks, as well as the effectiveness of \n","    different defense mechanisms, detection methods, and certification approaches.\n","    \n","    Key findings:\n","    \n","    1. The model is most vulnerable to PGD and Carlini-Wagner attacks, which achieved \n","       the highest success rates.\n","    \n","    2. Adversarial training proved to be the most effective defense mechanism, \n","       significantly improving model robustness against all tested attacks.\n","    \n","    3. Our adversarial example detector achieved good precision and recall, providing \n","       an additional layer of security.\n","    \n","    4. The model could be certified robust against small perturbations, but robustness \n","       guarantees decreased rapidly with increasing perturbation size.\n","    \n","    5. Overall security risk assessment indicates a Medium risk level, with specific \n","       areas for improvement identified.\n","    \n","    By implementing the recommended defense mechanisms, detection methods, and following \n","    the production guidelines, financial institutions can significantly improve the \n","    security of their machine learning models against adversarial attacks, helping to \n","    maintain the integrity of fraud detection systems and protect customer assets.\n","    \n","    This security evaluation should be repeated periodically, especially after model \n","    updates or when new attack methods are discovered, to ensure continuous protection \n","    against evolving threats.\n","    \n"]}]}]}