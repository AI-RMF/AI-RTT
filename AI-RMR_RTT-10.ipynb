{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvYPscrsGRPYh5KrjWbNMP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_szS4Q4_RpQa","executionInfo":{"status":"ok","timestamp":1728394251988,"user_tz":240,"elapsed":2843,"user":{"displayName":"Bobby Jenkins","userId":"18327594026683774342"}},"outputId":"cbb9c479-5d61-42a9-83e2-9c9c90b6cbc0"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Clean Document Classification: [{'label': 'LABEL_1', 'score': 0.591474175453186}]\n","\n","Fuzzed Document (10% noise) Classification: [{'label': 'LABEL_1', 'score': 0.5881022214889526}]\n","\n","Fuzzed Document (30% noise) Classification: [{'label': 'LABEL_1', 'score': 0.5162310600280762}]\n","\n","Fuzzed Document (Special Characters) Classification: [{'label': 'LABEL_1', 'score': 0.5515407919883728}]\n","\n","--- Analysis of Results ---\n","Clean Document Classification: [{'label': 'LABEL_1', 'score': 0.591474175453186}]\n","Fuzzed (10% noise) Document Classification: [{'label': 'LABEL_1', 'score': 0.5881022214889526}]\n","Fuzzed (30% noise) Document Classification: [{'label': 'LABEL_1', 'score': 0.5162310600280762}]\n","Fuzzed (Special Characters) Document Classification: [{'label': 'LABEL_1', 'score': 0.5515407919883728}]\n"]}],"source":["# Step 1: Import necessary libraries\n","import random\n","import string\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import pipeline\n","\n","# Step 2: Load pre-trained BERT model for text classification\n","# For simplicity, we'll use a BERT model fine-tuned for sequence classification\n","model_name = \"bert-base-uncased\"\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification for simplicity\n","\n","# Create a text classification pipeline\n","nlp_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n","\n","# Step 3: Define a clean contract document as an example input\n","clean_document = \"\"\"\n","This contract is made between ABC Corporation and the Department of Defense (DoD) on the 5th of May, 2024.\n","The contract covers the provision of cybersecurity services for a period of three years, with a total value of $5 million.\n","\"\"\"\n","\n","# Classify the clean document\n","clean_result = nlp_pipeline(clean_document)\n","print(\"Clean Document Classification:\", clean_result)\n","\n","# Step 4: Fuzzing - Generate Adversarial Inputs\n","\n","# Fuzzing function: Introduce random noise into the text\n","def fuzz_document(document, noise_level=0.1):\n","    \"\"\"Introduces random characters and noise into the document.\"\"\"\n","    document_chars = list(document)\n","    num_changes = int(noise_level * len(document_chars))\n","    for _ in range(num_changes):\n","        index = random.randint(0, len(document_chars) - 1)\n","        document_chars[index] = random.choice(string.punctuation + string.ascii_letters)\n","    return ''.join(document_chars)\n","\n","# Example 1: Fuzzed document with 10% noise\n","fuzzed_document_10 = fuzz_document(clean_document, noise_level=0.1)\n","\n","# Classify the fuzzed document with 10% noise\n","fuzzed_result_10 = nlp_pipeline(fuzzed_document_10)\n","print(\"\\nFuzzed Document (10% noise) Classification:\", fuzzed_result_10)\n","\n","# Example 2: Fuzzed document with 30% noise (higher corruption level)\n","fuzzed_document_30 = fuzz_document(clean_document, noise_level=0.3)\n","\n","# Classify the fuzzed document with 30% noise\n","fuzzed_result_30 = nlp_pipeline(fuzzed_document_30)\n","print(\"\\nFuzzed Document (30% noise) Classification:\", fuzzed_result_30)\n","\n","# Step 5: Special characters and malformed structure fuzzing\n","def inject_special_chars(document, injection_ratio=0.2):\n","    \"\"\"Injects special characters randomly into the document.\"\"\"\n","    document_chars = list(document)\n","    num_injections = int(injection_ratio * len(document_chars))\n","    for _ in range(num_injections):\n","        index = random.randint(0, len(document_chars) - 1)\n","        document_chars[index] = random.choice(string.punctuation)\n","    return ''.join(document_chars)\n","\n","# Example 3: Fuzzed document with special character injection (20%)\n","fuzzed_special_chars = inject_special_chars(clean_document, injection_ratio=0.2)\n","\n","# Classify the fuzzed document with special characters\n","fuzzed_result_special_chars = nlp_pipeline(fuzzed_special_chars)\n","print(\"\\nFuzzed Document (Special Characters) Classification:\", fuzzed_result_special_chars)\n","\n","# Step 6: Analyze Results\n","def analyze_results(clean_res, fuzzed_res_10, fuzzed_res_30, fuzzed_special_chars_res):\n","    print(\"\\n--- Analysis of Results ---\")\n","    print(\"Clean Document Classification:\", clean_res)\n","    print(\"Fuzzed (10% noise) Document Classification:\", fuzzed_res_10)\n","    print(\"Fuzzed (30% noise) Document Classification:\", fuzzed_res_30)\n","    print(\"Fuzzed (Special Characters) Document Classification:\", fuzzed_special_chars_res)\n","\n","# Run the analysis\n","analyze_results(clean_result, fuzzed_result_10, fuzzed_result_30, fuzzed_result_special_chars)\n"]}]}